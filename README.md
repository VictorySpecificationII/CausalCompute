# CausalCompute â€” First-Principles Sizing Engine (Steps 0â€“2)

> **Infrastructure decisions are derivations from physics and workload, not brand templates.**

This repository implements a vendor-neutral method to translate **AI training intent** into **physical requirements** using only SI units and explicit assumptions.

The engine answers:

**â€œWhat must exist in the real world to finish this training run on time?â€**

without assuming:
- a topology  
- a vendor  
- a rack layout  
- a product line  

---

## What the engine does

### Step 0 â€” Fundamentals (physics only)

From a brief:

- model size  
- tokens  
- deadline  
- algorithmic step size  

we derive **absolute invariants**:

- sustained FLOP/s required  
- minimum model-state bytes  
- dataset & checkpoint bandwidth  
- update payload per step  
- maximum allowed step time  

ğŸ‘‰ No cluster design is assumed here â€” only reality.

---

### Step 1 â€” Design closure

Introduce architecture choices:

- DP / TP / PP factorization  
- real efficiencies (Î·_compute, Î·_fabric)  
- communication model

and solve:

- memory per device  
- step time = compute + comm  
- smallest feasible GPU count (or test a fixed G)

ğŸ‘‰ This is the **causal bridge** from physics â†’ architecture.

---

### Step 2 â€” Power & Thermals

From the feasible design we compute:

- IT and facility power  
- heat production  
- airflow or coolant flow  
- optional rack sanity  
- energy over the run

ğŸ‘‰ This is the handoff toward mechanical & electrical design.

---

## Quick start

Install dependency:

```bash
pip install pyyaml
````

Run a brief:

```bash
python run_012.py briefs/13b.yaml
```

Narrative Step-0 explanation:

```bash
python run_012.py briefs/13b.yaml --story
```

Full debug bundles:

```bash
python run_012.py briefs/13b.yaml --debug
```

---

## Repository structure

```
first-principles/
â”œâ”€â”€ briefs/
â”‚   â”œâ”€â”€ template.yaml     â† how to describe a workload
â”‚   â”œâ”€â”€ 13b.yaml          â† example scenario
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ run_012.py            â† engine entrypoint (Steps 0â†’1â†’2)
â”‚
â”œâ”€â”€ step0_fundamentals/   â† physics, no topology
â”œâ”€â”€ step1_design/         â† DP/TP/PP closure
â””â”€â”€ step2_powerandthermals/
```

A **brief** is the contract between ML intent and infrastructure sizing.

---

## Design philosophy

### 1) Causality flows one way

```
Workload â†’ FLOPs â†’ Time â†’ Memory â†’ Communication
         â†’ Power â†’ Heat â†’ Flow â†’ Space
```

Nothing is guessed from brands.

---

### 2) Everything in SI

* bytes, seconds, bytes/s
* FLOP, FLOP/s
* watts, kg/s, mÂ³/s

Non-SI (CFM, LPM) are display only.

---

### 3) Explicit assumptions

Efficiencies and policies are parameters:

* Î·_compute â€” sustained vs peak math
* Î·_fabric â€” real collectives vs line rate
* Tok_per_step â€” algorithmic choice
* Î”T â€” mechanical design envelope

Change the brief â†’ the physical answer changes.

---

### 4) Topology is a decision, not an input

Step 0 does **not** assume:

* nodes
* racks
* networks

Step 1 introduces them only when required to close time and memory.

---

## What this is (and is not)

**This is:**

* a reference sizing engine
* a digital-twin seed
* a communication bridge between ML, EE, and ME

**This is not:**

* a vendor selector
* a BOM generator
* a CFD tool
* a scheduler

Those come later â€” after physics is satisfied.

---

## Example questions it can answer

* â€œHow many GPUs must exist at minimum?â€
* â€œIs this deadline even possible?â€
* â€œWhat coolant flow is implied by the workload?â€
* â€œHow much power does the facility need to commit?â€
* â€œWhat bandwidth must the fabric expose before topology?â€

---

## Extending beyond Step 2

Future layers can consume the Step-2 handoff:

* Step 3 â€” Networking topology
* Step 4 â€” Storage design
* Step 5 â€” Facilities zoning & transients
* Digital twin supervision

---

## Author

Built from the viewpoint of someone who has lived in:

* HPC & distributed systems
* control systems & thermodynamics
* motorsport engineering

Treating datacenters as **thermodynamic machines**, not SKU catalogs.
