# CausalCompute â€” First-Principles Sizing Engine (Steps 0â€“2)

> **Infrastructure decisions are derivations from physics and workload, not brand templates â€” and the predictions have been validated against real H100 InfiniBand training runs.**

This repository implements a vendor-neutral method to translate **AI training intent** into **physical requirements** using only SI units and explicit assumptions.

The engine answers:

**â€œWhat must exist in the real world to finish this training run on time?â€**

without assuming:
- a topology  
- a vendor  
- a rack layout  
- a product line  

---

## What the engine does

### Step 0 â€” Fundamentals (physics only)

From a brief:

- model size  
- tokens  
- deadline  
- algorithmic step size  

we derive **absolute invariants**:

- sustained FLOP/s required  
- minimum model-state bytes  
- dataset & checkpoint bandwidth  
- update payload per step  
- maximum allowed step time  

ğŸ‘‰ No cluster design is assumed here â€” only reality.

---

### Step 1 â€” Design closure

Introduce architecture choices:

- DP / TP / PP factorization  
- real efficiencies (Î·_compute, Î·_fabric)  
- communication model

and solve:

- memory per device  
- step time = compute + comm  
- smallest feasible GPU count (or test a fixed G)

ğŸ‘‰ This is the **causal bridge** from physics â†’ architecture.

---

### Step 2 â€” Power & Thermals

From the feasible design we compute:

- IT and facility power  
- heat production  
- airflow or coolant flow  
- optional rack sanity  
- energy over the run

ğŸ‘‰ This is the handoff toward mechanical & electrical design.

---

## Quick start

Install dependency:

```bash
pip install pyyaml
````

Run a brief:

```bash
python run_012.py briefs/13b.yaml
```

Narrative Step-0 explanation:

```bash
python run_012.py briefs/13b.yaml --story
```

Full debug bundles:

```bash
python run_012.py briefs/13b.yaml --debug
```

---

## Repository structure

```
first-principles/
â”œâ”€â”€ briefs/
â”‚   â”œâ”€â”€ template.yaml     â† how to describe a workload
â”‚   â”œâ”€â”€ 13b.yaml          â† example scenario
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ run_012.py            â† engine entrypoint (Steps 0â†’1â†’2)
â”‚
â”œâ”€â”€ step0_fundamentals/   â† physics, no topology
â”œâ”€â”€ step1_design/         â† DP/TP/PP closure
â””â”€â”€ step2_powerandthermals/
```

A **brief** is the contract between ML intent and infrastructure sizing.

---

## Design philosophy

### 1) Causality flows one way

```
Workload â†’ FLOPs â†’ Time â†’ Memory â†’ Communication
         â†’ Power â†’ Heat â†’ Flow â†’ Space
```

Nothing is guessed from brands.

---

### 2) Everything in SI

* bytes, seconds, bytes/s
* FLOP, FLOP/s
* watts, kg/s, mÂ³/s

Non-SI (CFM, LPM) are display only.

---

### 3) Explicit assumptions

Efficiencies and policies are parameters:

* Î·_compute â€” sustained vs peak math
* Î·_fabric â€” real collectives vs line rate
* Tok_per_step â€” algorithmic choice
* Î”T â€” mechanical design envelope

Change the brief â†’ the physical answer changes.

---

### 4) Topology is a decision, not an input

Step 0 does **not** assume:

* nodes
* racks
* networks

Step 1 introduces them only when required to close time and memory.

---

## What this is (and is not)

**This is:**

* a reference sizing engine
* a digital-twin seed
* a communication bridge between ML, EE, and ME

**This is not:**

* a vendor selector
* a BOM generator
* a CFD tool
* a scheduler

Those come later â€” after physics is satisfied.

---

## Example questions it can answer

* â€œHow many GPUs must exist at minimum?â€
* â€œIs this deadline even possible?â€
* â€œWhat coolant flow is implied by the workload?â€
* â€œHow much power does the facility need to commit?â€
* â€œWhat bandwidth must the fabric expose before topology?â€

---

## Validation

CausalCompute has been validated against real distributed training measurements on an H100 InfiniBand cluster.

Measured on Nebius H100 SXM nodes:

- 8 GPU step time: 0.037809 s  
- 16 GPU step time: 0.039018 s  

CausalCompute prediction:

- 8 GPU predicted: 0.037810 s  
- 16 GPU predicted: 0.038975 s  

Error:

- absolute step-time error: < 0.11%  
- scaling penalty error: < 4%  

This demonstrates that distributed training performance emerges correctly from:

- workload FLOPs
- sustained compute efficiency
- sustained fabric bandwidth
- communication overlap assumptions

without encoding vendor topology, product specifications, or empirical scaling curves.

The only calibrated inputs were sustained compute efficiency and sustained fabric bandwidth, both directly measured on the target system. No topology-specific heuristics, scaling curves, or vendor performance models were used.

Full validation details and reproducible commands are available in:

```bash
evidence/nebius_h100_ib_validation.md
```

This demonstrates that cluster sizing and distributed training performance can be derived from first principles and validated against real hardware.

---

## Evidence Structure

Validation artifacts are organized as follows:

```bash
evidence/
â”œâ”€â”€ nebius_h100_ib_validation.md â† validation report
â”œâ”€â”€ causalcompute_validation_backup/
â”‚ â”œâ”€â”€ allreduce_bw_node0.txt â† measured fabric bandwidth
â”‚ â”œâ”€â”€ ddp_8gpu.txt â† measured single-node step time
â”‚ â”œâ”€â”€ ddp_16gpu_node0.txt â† measured multi-node step time
â”‚ â””â”€â”€ *.py â† benchmark scripts
â”‚
â”œâ”€â”€ validate_dp8.out â† predicted 8 GPU result
â””â”€â”€ validate_dp16.out â† predicted 16 GPU result
```

This provides full traceability from measurement â†’ model â†’ prediction.


---

## Extending beyond Step 2

Future layers can consume the Step-2 handoff:

* Step 3 â€” Networking topology
* Step 4 â€” Storage design
* Step 5 â€” Facilities zoning & transients
* Digital twin supervision

---

## Author

Built from the viewpoint of someone who has lived in:

* HPC & distributed systems
* control systems & thermodynamics
* motorsport engineering

Treating datacenters as **thermodynamic machines**, not SKU catalogs.
